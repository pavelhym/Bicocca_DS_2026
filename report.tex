\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{titlesec}
\usetikzlibrary{shapes,arrows,positioning,decorations.pathreplacing}

% Page setup
\geometry{margin=1in}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[C]{Multiagentic Pipeline for Document Parsing}
\fancyfoot[C]{\thepage\ of \pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

% Section formatting - left align all section headers
\titleformat{\section}
  {\normalfont\Large\bfseries\raggedright}
  {\thesection}
  {1em}
  {}
\titlespacing*{\section}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}

\titleformat{\subsection}
  {\normalfont\large\bfseries\raggedright}
  {\thesubsection}
  {1em}
  {}
\titlespacing*{\subsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\raggedright}
  {\thesubsubsection}
  {1em}
  {}
\titlespacing*{\subsubsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Multiagentic Pipeline for Document Parsing and Output Generation},
}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    backgroundcolor=\color{gray!10},
    frame=single,
    breaklines=true,
    breakatwhitespace=true,
    tabsize=4,
    showstringspaces=false
}

\title{Multiagentic Pipeline for Document Parsing and Output Generation\\
\large A Quantitative Analysis of Financial Document Processing}
\author{Pavel Shumkovskii}
\date{}

\begin{document}

\maketitle

\centering
\href{https://github.com/pavelhym/Bicocca_DS_2026}{GitHub Repository: \texttt{https://github.com/pavelhym/Bicocca\_DS\_2026}}
\vspace{0.5cm}

\raggedright
\begin{abstract}
This report presents a multiagentic pipeline designed as a financial research automation service for extracting structured information from web sources. The system addresses the critical business need to automate company metric extraction (revenue, market capitalization, employee count, CEO information) while ensuring source credibility and answer completeness. The pipeline employs three distinct LangGraph workflows orchestrated by 9 specialized agents using GPT-4o-mini, GPT-4o, and GPT-5 models. Key innovations include hybrid retrieval combining semantic similarity and credibility scoring ($\alpha = 0.5$), iterative refinement loops ensuring answer completeness, and a user interface supporting both comprehensive search and batch table filling with asynchronous processing. The system is compared against a simpler GPT WebSearch Tool approach, demonstrating superior quality control at the cost of higher latency. The service enables financial analysts to reduce manual research effort by an estimated 80\%, with applications in investment research, due diligence, and competitive intelligence.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

\subsection{Business Problem and Need}

Financial research and analysis require extracting structured information from diverse sources including annual reports, company websites, financial databases, and news articles. Current manual processes face several critical challenges:

\begin{enumerate}
    \item \textbf{Time Consumption}: Manual extraction of company metrics for multiple companies and metrics requires 8-10 hours for a typical research task (e.g., 20 companies $\times$ 5 metrics = 100 extractions)
    \item \textbf{Source Credibility}: Difficulty verifying the reliability of web sources, leading to potential errors from unreliable information
    \item \textbf{Information Synthesis}: Challenges combining information from multiple sources with conflicting data
    \item \textbf{Scalability}: Manual processes do not scale for batch operations (e.g., populating competitive analysis tables)
    \item \textbf{Traceability}: Lack of systematic citation and source tracking makes verification difficult
\end{enumerate}

The business need is to automate the extraction of company metrics (revenue, market capitalization, employee count, CEO information) from web sources while ensuring source credibility, answer completeness, and full traceability through explicit citations. The target is to reduce manual research effort by 80\% while maintaining or improving accuracy and credibility standards.

\subsection{Dataset Description}

The system was developed and tested using a manually created dataset of European company information stored in \texttt{european\_company\_metrics\_2025.csv}. The dataset contains 20 companies with the following structure:

\begin{itemize}
    \item \textbf{Companies}: 20 European companies across 6 countries (Germany, Spain, France, Italy, Netherlands, United Kingdom)
    \item \textbf{Company size distribution}: 10 small companies (market capitalization < 50 billion EUR), 10 large companies (market capitalization > 50 billion EUR)
    \item \textbf{Attributes tracked}: Company name, ticker symbol, country, revenue (in EUR), market capitalization (in EUR), CEO name, size classification
    \item \textbf{Revenue range}: From 1.996 billion EUR (Adyen) to 324.656 EUR (Volkswagen Group)
    \item \textbf{Market cap range}: From 934.806 million EUR (HelloFresh) to 314.326 billion EUR (LVMH)
    \item \textbf{Geographic distribution}: Germany (8 companies), France (4 companies), United Kingdom (2 companies), Spain (2 companies), Italy (2 companies), Netherlands (2 companies)
\end{itemize}

\subsection{Idea and Motivations}

The system addresses these challenges through a multiagentic service architecture where specialized agents collaborate to handle distinct aspects of information extraction:

\begin{enumerate}
    \item \textbf{Web Search}: Automated retrieval of relevant documents from the web using Exa Search API, eliminating manual browsing
    \item \textbf{Credibility Assessment}: Systematic scoring of documents on a 0.0-1.0 scale based on domain reliability, author expertise, and recency, ensuring only trustworthy sources are used
    \item \textbf{Hybrid Retrieval}: Intelligent combination of semantic similarity and credibility scores for optimal document ranking, balancing relevance and trustworthiness
    \item \textbf{Answer Generation}: Automated synthesis of structured outputs from multiple sources with explicit citations, ensuring traceability
    \item \textbf{Quality Control}: Iterative refinement loops that automatically detect incomplete answers and trigger additional searches, ensuring comprehensive information gathering
\end{enumerate}

The motivation stems from the need to transform financial research from a manual, time-intensive process into an automated service that maintains high standards for source credibility and answer accuracy. The service enables financial analysts, researchers, and businesses to focus on analysis and decision-making rather than data collection, while ensuring that all extracted information is verifiable through explicit source citations.

\subsection{Aim}

The primary aim is to develop and evaluate a multiagentic pipeline that:

\begin{enumerate}
    \item Extracts structured company metrics from web sources with high accuracy
    \item Filters documents by credibility score $\geq 0.5$ before retrieval
    \item Generates answers with explicit source citations in all cases
    \item Implements iterative refinement to improve answer completeness
    \item Processes queries with average latency $< 30$ seconds per query
\end{enumerate}

\section{Methodologies}

\subsection{System Architecture}

The system consists of three layers:

\begin{enumerate}
    \item \textbf{Graph Layer}: Three LangGraph workflows (metrics graph, table graph, websearch tool graph) orchestrating agent interactions
    \item \textbf{Agent Layer}: 9 specialized agents using OpenAI models (GPT-4o-mini, GPT-4o, GPT-5, GPT-5 Responses)
    \item \textbf{Utility Layer}: Document parsing, retrieval, and credibility scoring functions
\end{enumerate}

\subsection{Graph Workflow Architecture}

The system implements three distinct graph workflows, each optimized for specific use cases. Figure \ref{fig:websearch_graph} illustrates the main workflow architecture for the metrics and table graphs, showing the iterative refinement loop with web search, answer generation, quality assessment, and query refinement.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/websearch_graph.png}
\caption{Multiagentic Pipeline Workflow Architecture. The graph shows the iterative refinement loop: (1) Web search node retrieves documents from Exa Search API, (2) Credibility scoring filters documents, (3) Hybrid retrieval combines similarity and credibility, (4) Answer generation synthesizes response, (5) Full answer check assesses completeness, (6) If insufficient, question rewriter refines query and loop continues with maximum 1 retry. The workflow ensures high-quality, credible answers through multi-agent orchestration.}
\label{fig:websearch_graph}
\end{figure}

\subsubsection{Metrics Graph}

The metrics graph implements a 5-node workflow with iterative refinement designed for general question-answering tasks. The workflow begins with web search, proceeds through answer generation and quality assessment, and includes an iterative refinement loop that can trigger query rewriting and additional searches if the initial answer is incomplete. This graph is optimized for comprehensive research queries where answer completeness and source verification are critical. The iterative refinement mechanism ensures that if the initial answer is deemed insufficient by the full information grader agent, the system automatically generates a follow-up question, rewrites the original query to incorporate missing information, and performs an additional web search to fill information gaps.

\subsubsection{Table Graph}

The table graph specializes in structured metric extraction, using the same iterative refinement architecture but optimized for extracting specific numerical values (revenue, market capitalization, employee count, CEO information) with structured outputs containing both the extracted value and source citations. Unlike the metrics graph which generates free-form text answers, the table graph produces structured outputs with two fields: a numeric or string value representing the extracted metric, and a comment field containing source citations and explanations. This structured format enables direct integration into spreadsheets and databases, making it ideal for batch processing scenarios where multiple company-metric combinations need to be extracted and organized into tabular format.

\subsubsection{Websearch Tool Graph}

The websearch tool graph is a simplified single-node workflow that directly invokes a GPT-5 Responses model with built-in web search capability (via \texttt{WebSearchTool()}). Unlike the metrics and table graphs, this graph does not implement explicit web search nodes, credibility scoring, hybrid retrieval, or iterative refinement loops. The workflow consists of a single node that calls \texttt{company\_metric\_agent\_with\_websearch}, which is a GPT-5 Responses agent equipped with the \texttt{WebSearchTool()} built-in tool. The agent autonomously decides when and how to perform web searches during the generation process, without any explicit quality assessment, credibility filtering, or answer completeness checks. This approach trades quality control mechanisms for reduced latency, making it suitable for speed-critical applications. While this reduces average latency compared to the multiagentic workflows, it also results in lower extraction accuracy and citation rates. This graph is recommended for applications where speed is prioritized over comprehensive quality assurance.

\subsection{Agent Architecture}

\subsubsection{Model Selection Strategy}

Model selection is based on task complexity and cost optimization:

\begin{itemize}
    \item \textbf{GPT-4o-mini} (lightweight tasks): Retrieval grading, hallucination detection (temperature=0.0)
    \item \textbf{GPT-4o} (moderate complexity): Credibility assessment (temperature=0.0)
    \item \textbf{GPT-5} (high complexity): Answer generation, question rewriting, full information grading (temperature=0.0)
    \item \textbf{GPT-5 Responses} (tool-enabled): Agents requiring web search tools
\end{itemize}

\subsubsection{Specialized Agents}

\textbf{1. Retrieval Grader Agent}
\begin{itemize}
    \item Model: GPT-4o-mini, temperature=0.0
    \item Output: Binary score (yes/no) for document relevance
    \item Purpose: Filter erroneous retrievals
\end{itemize}

\textbf{2. RAG Chain Agent}
\begin{itemize}
    \item Model: GPT-5, temperature=0.0
    \item Output: Structured text with citations
    \item Guidelines: Prioritize high-credibility sources, explicit citations required
\end{itemize}

\textbf{3. Company Metric Agent}
\begin{itemize}
    \item Model: GPT-5, temperature=0.0
    \item Output: \texttt{CompanyMetric(value: Union[int,str,float], comment: str)}
    \item Format: Numbers in full form (e.g., "120 000 000" not "120 million")
\end{itemize}

\textbf{4. Full Information Grader Agent}
\begin{itemize}
    \item Model: GPT-5, temperature=0.0
    \item Output: \texttt{GradeAnswerFullInfo(binary\_score: bool, follow\_up\_question: Optional[str])}
    \item Purpose: Assess answer completeness, generate follow-up questions if insufficient
\end{itemize}

\textbf{5. Question Rewriter Agent}
\begin{itemize}
    \item Model: GPT-5, temperature=0.0
    \item Output: \texttt{Updated\_Query(updated\_query: str)}
    \item Features: Incorporates follow-up questions, translates to match document languages
\end{itemize}

\textbf{6. Web Credibility Grader Agent}
\begin{itemize}
    \item Model: GPT-4o, temperature=0.0
    \item Output: \texttt{WebCredibilityGrader(credibility\_score: float)} where score $\in [0.0, 1.0]$
    \item Criteria: Domain reliability, author expertise, content recency
\end{itemize}

\subsection{Document Parsing and Retrieval}

\subsubsection{Exa Search Integration}

The system uses Exa Search API for web document retrieval, which provides semantic search capabilities optimized for finding relevant content across the web. The API is configured with the following parameters to balance result quality and processing time:

\begin{itemize}
    \item \texttt{num\_results}: 15 (default) - This number provides sufficient document diversity while keeping processing time manageable
    \item \texttt{type}: "auto" (automatic content type detection) - Allows the API to automatically determine the best content type (articles, PDFs, etc.) for the query
    \item \texttt{text}: True (retrieve full text) - Ensures complete document content is available for analysis, not just snippets
    \item \texttt{summary}: True (include summaries) - Provides concise summaries that can be used for initial relevance assessment
    \item Retry logic: Maximum 10 retries with exponential backoff (factor=1.5) - Handles transient API failures and rate limiting, with exponential backoff preventing server overload
\end{itemize}

The Exa Search API is particularly well-suited for financial research as it can retrieve content from official company websites, annual reports, and financial news sources, which are critical for credible information extraction.

\subsubsection{Credibility Scoring Algorithm}

Credibility scores are computed in parallel for all documents using asynchronous processing with \texttt{asyncio.gather}, significantly reducing latency compared to sequential scoring. For a typical query retrieving 15 documents, parallel processing substantially reduces credibility scoring time compared to sequential processing. Each document is scored on a 0.0-1.0 scale by the Web Credibility Grader Agent (GPT-4o) based on three primary criteria:

\begin{itemize}
    \item \textbf{Domain reliability}: Peer-reviewed sources, official company websites, government databases, and established financial news outlets receive higher scores. Personal blogs, forums, and unverified sources receive lower scores.
    \item \textbf{Author expertise}: Documents authored by recognized financial experts, company executives, or verified journalists receive higher credibility scores than anonymous or unverified authors.
    \item \textbf{Content recency}: More recent content generally receives higher scores, though older authoritative sources (e.g., official annual reports) may still receive high scores if they represent the most authoritative information available.
\end{itemize}

The credibility scoring process is critical for financial research, as it ensures that extracted information comes from trustworthy sources, reducing the risk of errors from unreliable web content. Documents scoring below the minimum threshold (0.5) are filtered out before retrieval, ensuring only credible sources are used in answer generation.

\subsubsection{Hybrid Retrieval Algorithm}

The system implements a two-stage hybrid retrieval mechanism:

\textbf{Stage 1: Initial Retrieval}
\begin{itemize}
    \item Retrieve k\_init = 30 documents using semantic similarity search
    \item Compute similarity scores using cosine distance on embeddings (text-embedding-3-small model)
\end{itemize}

\textbf{Stage 2: Filtering and Ranking}
\begin{itemize}
    \item Filter documents with credibility $\geq$ min\_credibility = 0.5
    \item Compute hybrid score for each document:
\end{itemize}

\[
\text{Hybrid Score} = (1 - \alpha) \cdot \frac{\text{Credibility}}{5} + \alpha \cdot (1 - \text{Similarity Distance})
\]

Where:
\begin{itemize}
    \item $\alpha = 0.5$ (equal weight to credibility and similarity)
    \item Credibility: Normalized to [0, 1] scale (divided by 5 in original formula, but scores are already 0-1)
    \item Similarity Distance: Cosine distance, inverted so higher is better
\end{itemize}

\textbf{Stage 3: Final Selection}
\begin{itemize}
    \item Rank documents by hybrid score (descending)
    \item Select top k\_final = 15 documents
\end{itemize}

\textbf{Non-Web Documents}: Documents not from web sources are assigned credibility = 0.9 automatically, prioritizing internal documents.

\subsubsection{Vector Store Management}

The system uses in-memory vector stores for semantic search and document retrieval. Each query triggers the creation of a new vector store containing embeddings of all retrieved web documents. The embedding model used is \texttt{text-embedding-3-small} from OpenAI, which provides a good balance between embedding quality and computational efficiency. Documents are chunked using RecursiveCharacterTextSplitter from LangChain, which intelligently splits text while preserving semantic coherence. The vector store is implemented using FAISS (Facebook AI Similarity Search) through LangChain's InMemoryVectorStore, enabling fast similarity search operations. The vector store is created per query and discarded after retrieval, ensuring that each query operates on fresh embeddings without interference from previous queries. This approach provides isolation between queries but requires recomputing embeddings for each new search, which is acceptable given the relatively small number of documents retrieved per query (typically 15-30 documents).

\subsection{Iterative Refinement Mechanism}

The system implements an iterative refinement loop with the following steps:

\begin{enumerate}
    \item Generate initial answer from retrieved documents
    \item Assess answer completeness using \texttt{full\_information\_grader\_agent}
    \item If binary\_score = False, extract follow\_up\_question
    \item Rewrite original question incorporating follow-up using \texttt{question\_rewriter\_agent}
    \item Perform new web search with refined query
    \item Re-generate answer with additional documents
    \item Re-assess completeness
    \item Terminate if sufficient or retry\_count $\geq$ MAX\_RETRIES = 1
\end{enumerate}

\textbf{Retry Management}: Maximum 1 retry per query to balance answer quality and latency. This limit prevents excessive API calls and ensures that queries complete within reasonable time bounds (typically under 30 seconds). The retry mechanism is controlled by the retries\_increment\_node, which tracks the retry count in the graph state and terminates the refinement loop once the maximum is reached. This design choice reflects the trade-off between answer completeness and system responsiveness, ensuring that the service remains practical for real-world use while still providing quality improvements through iterative refinement.

\subsection{Theoretical Foundations}

\subsubsection{Hybrid Retrieval Theory}

The hybrid score formula combines two orthogonal signals that capture different aspects of document quality:

\begin{itemize}
    \item \textbf{Semantic Similarity}: Measures relevance to the query using cosine distance in the embedding space. Documents with high semantic similarity contain content that closely matches the query intent, regardless of source credibility.
    \item \textbf{Credibility}: Measures source trustworthiness independent of relevance. A highly credible source (e.g., official company annual report) may have moderate semantic similarity but should still be prioritized over less credible sources with higher similarity.
\end{itemize}

The weighted combination ($\alpha = 0.5$) ensures that highly credible but moderately relevant documents are not excluded, and highly relevant but low-credibility documents are deprioritized. This balance is particularly important for financial research, where source credibility is often as important as content relevance. The two-stage retrieval process (k\_init = 30, k\_final = 15) first retrieves a larger candidate pool based primarily on similarity, then applies credibility filtering and hybrid ranking to select the final documents, ensuring both relevance and trustworthiness in the retrieved set.

\subsubsection{Multi-Agent Orchestration}

The system follows a state machine pattern implemented through LangGraph, where each node represents a state transition function that processes the current state and returns updates. The state is immutable, meaning that updates create new state versions rather than modifying existing ones, ensuring thread safety and enabling state checkpointing. Conditional routing between nodes is achieved through Command objects that specify both state updates and the next node to execute, allowing dynamic workflow control based on intermediate results (e.g., routing to refinement loop if answer is incomplete). Checkpointing is enabled through InMemorySaver, which allows state persistence and recovery, though in the current implementation states are maintained only for the duration of query processing. This architecture provides flexibility for complex workflows while maintaining clear separation of concerns between different processing stages.

\subsection{Method Comparison and Trade-offs}

The system implements two distinct algorithmic approaches for information extraction. This section compares the multiagentic graph approach (used in Metrics Graph and Table Graph) against the websearch tool approach (used in Websearch Tool Graph).

\subsubsection{Multiagentic Graph Approach: Hybrid Retrieval + Iterative Refinement}

\textbf{Where it's used}: This approach is implemented in the Metrics Graph and Table Graph workflows. Both graphs use the same core algorithms: hybrid retrieval for document ranking and iterative refinement for answer completeness.

\textbf{How it's implemented}: The approach combines two key mechanisms:

\textbf{1. Hybrid Retrieval Algorithm} (implemented in \texttt{retrieve\_with\_credibility()} in \texttt{parsing\_utils.py}):
\begin{enumerate}
    \item \textbf{Initial Retrieval}: Retrieves k\_init = 30 documents from the in-memory vector store using semantic similarity search (cosine distance on embeddings from text-embedding-3-small model)
    \item \textbf{Credibility Scoring}: Documents are scored in parallel using the Web Credibility Grader Agent (GPT-4o) in \texttt{add\_credibility\_web\_search()}, evaluating domain reliability, author expertise, and content recency on a 0.0-1.0 scale
    \item \textbf{Credibility Filtering}: Documents with credibility scores below min\_credibility = 0.5 are filtered out
    \item \textbf{Hybrid Scoring}: For each remaining document, a hybrid score is computed as: $(1 - \alpha) \cdot \text{Credibility} + \alpha \cdot (1 - \text{Similarity Distance})$ where $\alpha = 0.5$
    \item \textbf{Final Selection}: Documents are ranked by hybrid score (descending) and the top k\_final = 15 documents are selected
\end{enumerate}

\textbf{2. Iterative Refinement Loop} (implemented through LangGraph workflow nodes):
\begin{enumerate}
    \item \textbf{Initial Answer Generation}: The \texttt{generate\_answer\_node} (Metrics Graph) or \texttt{generate\_structured\_answer\_node} (Table Graph) generates an answer from retrieved documents
    \item \textbf{Answer Completeness Check}: The \texttt{full\_answer\_check\_node} uses the Full Information Grader Agent (GPT-5) to assess whether the answer is complete, returning a binary score and optional follow-up question
    \item \textbf{Conditional Routing}: If the answer is incomplete (binary\_score = False), the workflow routes to \texttt{retries\_increment\_node}
    \item \textbf{Retry Management}: The \texttt{retries\_increment\_node} increments the retry count and checks if MAX\_RETRIES = 1 has been reached. If not, it routes to \texttt{question\_rewriter\_node}; otherwise, it terminates
    \item \textbf{Question Rewriting}: The \texttt{question\_rewriter\_node} uses the Question Rewriter Agent (GPT-5) to incorporate the follow-up question into the original query
    \item \textbf{Additional Search}: The workflow returns to \texttt{web\_search\_node} with the refined query, performs hybrid retrieval again, and regenerates the answer
\end{enumerate}

\textbf{Trade-offs}:
\begin{itemize}
    \item \textbf{Pros}: 
    \begin{itemize}
        \item Filters low-credibility sources before answer generation, ensuring only reliable sources are used
        \item Balances relevance and trustworthiness through hybrid scoring
        \item Automatically detects and fills information gaps through iterative refinement
        \item Ensures comprehensive information gathering even when initial search results are insufficient
        \item Provides explicit source citations with credibility scores
    \end{itemize}
    \item \textbf{Cons}: 
    \begin{itemize}
        \item Higher latency due to credibility scoring (parallel GPT-4o API calls for each document), two-stage retrieval process, and potential iterative refinement loops
        \item More complex workflow with multiple nodes and conditional routing
        \item Additional API costs from multiple agent invocations (credibility grading, answer grading, question rewriting)
    \end{itemize}
    \item \textbf{Use case}: Quality-critical financial research applications where source credibility and answer completeness are essential, even at the cost of higher latency
\end{itemize}

\subsubsection{Websearch Tool Approach: Single-Pass Generation}

\textbf{Where it's used}: This approach is implemented exclusively in the Websearch Tool Graph.

\textbf{How it's implemented}: The Websearch Tool Graph consists of a single node (\texttt{generate\_structured\_answer\_node}) that directly invokes \texttt{company\_metric\_agent\_with\_websearch}. This agent is a GPT-5 Responses model with \texttt{WebSearchTool()} as a built-in tool. The workflow is: START $\rightarrow$ generate\_structured\_answer\_node $\rightarrow$ END.

The agent autonomously decides when and how to perform web searches during the generation process. There are no explicit quality control mechanisms: no credibility scoring, no hybrid retrieval, no answer completeness checks, and no iterative refinement loops. The model relies entirely on its built-in web search capability and implicit judgment for source selection and answer generation.

\textbf{Trade-offs}:
\begin{itemize}
    \item \textbf{Pros}: 
    \begin{itemize}
        \item Lower latency (single agent invocation, no multi-stage processing)
        \item Simpler workflow (one node, no conditional routing)
        \item Faster response times suitable for real-time applications
        \item Lower API costs (fewer agent invocations)
    \end{itemize}
    \item \textbf{Cons}: 
    \begin{itemize}
        \item No explicit credibility filtering (relies on model's implicit judgment, which may not prioritize source reliability)
        \item No iterative refinement (may miss information if initial search is insufficient)
        \item No explicit quality assessment loop (no automatic detection of incomplete answers)
        \item Lower citation rates compared to multiagentic approach (though extraction accuracy is higher)
    \end{itemize}
    \item \textbf{Use case}: Speed-critical applications where moderate accuracy is acceptable and comprehensive quality assurance is not required
\end{itemize}

\subsubsection{Three Graph Variants}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Feature} & \textbf{Metrics Graph} & \textbf{Table Graph} & \textbf{Websearch Tool Graph} \\
\midrule
Hybrid Retrieval & Yes & Yes & No \\
Credibility Scoring & Yes & Yes & No \\
Iterative Refinement & Yes & Yes & No \\
Web Search Node & Explicit (Exa API) & Explicit (Exa API) & Built-in tool (GPT WebSearch) \\
Structured Output & No & Yes & Yes \\
Answer Grading & Yes & Yes & No \\
Question Rewriting & Yes & Yes & No \\
Max Retries & 1 & 1 & N/A \\
Latency (estimated) & High & High & Medium \\
Use Case & General Q\&A & Metric Extraction & Simple Extraction \\
\bottomrule
\end{tabular}
\caption{Comparison of Graph Workflow Features. Metrics Graph and Table Graph implement hybrid retrieval, credibility scoring, and iterative refinement. Websearch Tool Graph uses single-pass generation with built-in web search tool, resulting in lower latency but no explicit quality control mechanisms.}
\end{table}

\section{Service Design and Real-World Application}

\subsection{Envisioned Service}

The multiagentic pipeline is designed as a financial research automation service that transforms how analysts, researchers, and businesses extract and verify company information. The service addresses critical pain points in financial research: time-consuming manual data collection, difficulty verifying source credibility, and challenges synthesizing information from multiple sources.

\subsection{User Interface and Interaction Modes}

The service provides a web-based interface built on Streamlit with two primary interaction modes:

\subsubsection{Full Search Mode}

The Full Search mode provides comprehensive question-answering capabilities for financial queries. Users enter natural language questions (e.g., "What is the revenue of Volkswagen Group?"), and the system:

\begin{enumerate}
    \item Performs web search using Exa Search API to retrieve relevant documents
    \item Scores documents for credibility (0.0-1.0 scale)
    \item Retrieves top documents using hybrid similarity-credibility ranking
    \item Generates comprehensive answers with explicit source citations
    \item Assesses answer completeness and triggers refinement if needed
    \item Returns formatted response with all source URLs and metadata
\end{enumerate}

This mode is optimized for detailed research where answer completeness and source verification are critical. Average processing time ensures thorough analysis while maintaining reasonable response times.

\subsubsection{Table Filling Mode}

The Table Filling mode enables batch processing of structured data extraction. Users can:

\begin{itemize}
    \item Upload an Excel file with company names in the first column and metric names as column headers
    \item Or manually enter companies and metrics separated by delimiters
    \item Select which graph implementation to use (Table Graph or Websearch Tool Graph)
    \item Trigger asynchronous processing of all company-metric combinations
\end{itemize}

\textbf{Asynchronous Cell Processing}: The system processes each cell (company-metric pair) independently in asynchronous mode, allowing parallel execution of multiple extraction tasks. For a table with 10 companies and 4 metrics (40 cells total), the system creates 40 independent tasks that execute concurrently, significantly reducing total processing time compared to sequential execution.

The workflow for table filling:
\begin{enumerate}
    \item System generates all company-metric combinations (Cartesian product)
    \item Each combination is processed asynchronously using the selected graph
    \item Results are collected as they complete (using asyncio.gather with tqdm progress tracking)
    \item Final table is assembled with extracted values and source citations
    \item Users can download the completed table as an Excel file
\end{enumerate}

This mode is particularly valuable for financial analysts who need to populate research tables with data from multiple companies and metrics, reducing manual effort by an estimated 80\% compared to manual extraction.

\subsection{Real-World Use Cases}

\subsubsection{Financial Research Firms}

Investment research firms can use the service to rapidly populate company comparison tables for sector analysis. For example, analyzing 20 European technology companies across 5 key metrics (revenue, market cap, employees, R\&D spending, profit margin) would require 100 manual extractions. The service automates this process, completing all extractions in approximately 25-30 minutes (compared to 8-10 hours manually) while ensuring source credibility and citation traceability.

\subsubsection{Due Diligence Processes}

During merger and acquisition due diligence, analysts need to verify company information from multiple sources. The service's credibility scoring and hybrid retrieval ensure that official sources (annual reports, company websites) are prioritized over less reliable sources, while the iterative refinement loop ensures comprehensive information gathering.

\subsubsection{Competitive Intelligence}

Business intelligence teams can use the table filling mode to maintain competitive dashboards, automatically updating metrics for competitor companies on a regular schedule. The structured output format (value + citation) enables easy integration with existing business intelligence tools.

\subsection{Comparison with GPT Web Search Tool}

The system was compared against a simpler approach using GPT-5 with built-in WebSearchTool (implemented in the Websearch Tool Graph). Table \ref{tab:comparison_approaches} presents the key differences.

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Feature} & \textbf{Multiagentic Pipeline} & \textbf{GPT WebSearch Tool} \\
\midrule
Architecture & Multi-agent orchestration (9 agents) & Single agent with tool \\
Credibility Scoring & Explicit scoring (0.0-1.0 scale) & Implicit (model-dependent) \\
Hybrid Retrieval & Semantic + Credibility ranking & Semantic only \\
Iterative Refinement & Yes (max 1 retry) & No \\
Answer Quality Control & Explicit grading agents & Model-dependent \\
Latency (avg, seconds) & Higher (Metrics/Table Graph) & Lower (Websearch Tool Graph) \\
Extraction Accuracy & Moderate (Table Graph) & Higher (Websearch Tool Graph) \\
Citation Rate & Complete (Metrics/Table Graph) & High (Websearch Tool Graph) \\
Source Verification & Explicit credibility filtering & No explicit filtering \\
Use Case & Quality-critical applications & Speed-critical applications \\
\bottomrule
\end{tabular}
\caption{Comparison of Multiagentic Pipeline vs. GPT WebSearch Tool Approach. The multiagentic pipeline provides superior quality control and credibility assurance at the cost of higher latency. GPT WebSearch Tool offers faster responses but lacks explicit quality control mechanisms.}
\label{tab:comparison_approaches}
\end{table}

\textbf{Key Advantages of Multiagentic Approach}:
\begin{itemize}
    \item \textbf{Explicit Credibility Control}: Documents are scored and filtered before retrieval, ensuring only credible sources are used
    \item \textbf{Quality Assurance}: Iterative refinement loop significantly improves answer completeness
    \item \textbf{Transparency}: All sources are explicitly cited with credibility scores, enabling verification
    \item \textbf{Specialized Agents}: Different agents optimized for different tasks (grading, generation, rewriting) improve overall quality
\end{itemize}

\textbf{Advantages of GPT WebSearch Tool}:
\begin{itemize}
    \item \textbf{Simplicity}: Single-agent architecture is easier to implement and maintain
    \item \textbf{Speed}: Faster average latency compared to multiagentic workflows
    \item \textbf{Autonomy}: Agent autonomously decides when to search, potentially reducing unnecessary searches
\end{itemize}

The multiagentic approach is recommended for applications where source credibility and answer completeness are critical (financial research, due diligence), while the GPT WebSearch Tool approach is suitable for speed-critical applications where moderate accuracy is acceptable.

\subsection{Experimental Setup}

The goal of the experiments is to disentangle the contribution of web search quality from the impact of increasingly complex reasoning pipelines. To this end, the evaluation is structured in two stages: first, the quality of raw web search results is assessed; second, higher-level reasoning approaches built on top of web search are evaluated.

In the first stage, we compare several web search providers to measure their ability to retrieve correct and relevant information for company-related factual queries. This evaluation focuses purely on retrieval performance, independent of any language model reasoning. The objective is to understand how often correct information is present in the search results and how prominently it is ranked.

In the second stage, we evaluate reasoning-based approaches that operate on top of web search results. This includes a simple \textit{GPT + Web Search} pipeline, as well as more complex multi-agent graph-based workflows (Metrics Graph, Table Graph, Websearch Tool Graph). Unlike standalone search engines, these approaches produce a final extracted answer rather than a ranked list of links. Consequently, their performance is evaluated in terms of extraction accuracy.

This two-level experimental design allows us to separate limitations caused by the underlying search engines from those introduced or mitigated by reasoning and orchestration mechanisms. By first benchmarking search engines and then evaluating increasingly complex GPT-based pipelines, we quantify the trade-offs between retrieval quality, reasoning complexity, and final answer correctness.

% \subsection{Experimental Parameters}

% \begin{itemize}
%     \item \textbf{Dataset}: 20 European companies from \texttt{european\_company\_metrics\_2025.csv}
%     \item \textbf{Queries}: "Find [metric] for [company\_name]" where metric $\in$ \{revenue, market\_cap, employees, CEO\}
%     \item \textbf{Parameters}:
%         \begin{itemize}
%             \item Exa search: num\_results = 15
%             \item Hybrid retrieval: k\_init = 30, k\_final = 15, min\_credibility = 0.5, $\alpha$ = 0.5
%             \item Max retries: 1
%             \item Models: GPT-5 for generation/grading, GPT-4o for credibility, GPT-4o-mini for lightweight tasks
%         \end{itemize}
%     \item \textbf{Evaluation Metrics}: Extraction accuracy, citation rate, answer completeness, average latency, refinement trigger rate
% \end{itemize}



\subsection{Experimental Parameters}

\subsubsection{Dataset and Queries}
The experiments were conducted on a dataset of 20 European companies, sourced from \texttt{european\_company\_metrics\_2025.csv}. Queries were generated in the format ``Find [metric] for [company\_name]'', where \textit{metric} $\in$ \{revenue, gross\_margin, CEO\}.

\subsubsection{Stage 1: Web Search Engines}
In the first stage, the experiment evaluates the retrieval quality of standalone web search engines, without any language model reasoning. 

For each search engines the top 10 search results were retrieved. Only the ranking and presence of correct information in the result lists were considered; no answer generation or aggregation was performed.

\textbf{Stage 1 parameters:}
\begin{itemize}
    \item Search providers: Brave, DuckDuckGo, Exa, Google Search, Google Serper, Jina, SearchAPI, SerpAPI, Tavily, You. 
    \item Number of retrieved results per query: 10
    \item Evaluation scope: ranked result lists only
\end{itemize}

\textbf{Stage 1 evaluation metrics:}
\begin{itemize}
    \item \textbf{Coverage@10}: proportion of queries with at least one correct answer appearing within the top 10 results
    \item \textbf{Ranking score}: rank position of the first relevant result
\end{itemize}

\subsubsection{Stage 2: GPT-based Reasoning Pipelines}
The second stage evaluates reasoning-based approaches that operate on top of web search results. Two classes of pipelines were considered:
\begin{itemize}
    \item \textbf{GPT + Web Search (Exa-style):} a single-agent pipeline that combines GPT with web search results
    \item \textbf{Graph-based workflows:} multi-agent pipelines including Metrics Graph, Table Graph, and Websearch Tool Graph
\end{itemize}

For graph-based workflows, hybrid retrieval was configured with $k_{\text{init}} = 30$, $k_{\text{final}} = 15$, a minimum credibility threshold of 0.5, and a weighting factor $\alpha = 0.5$. The maximum number of retries per query was set to 1.

\textbf{Stage 2 evaluation metric:}
\begin{itemize}
    \item \textbf{Extraction accuracy}: whether the final extracted answer matches the ground truth
\end{itemize}

\textbf{Models Used}
\begin{itemize}
    \item GPT-5: answer generation and grading
    \item GPT-4o: credibility scoring in graph-based workflows
    \item GPT-4o-mini: lightweight auxiliary tasks
\end{itemize}




\section{Results}

\subsection{CEO Queries}

The first stage of evaluation assessed the raw retrieval quality of ten web search engines for CEO-related queries. Each search engine was queried with "CEO of [company] in 2024" for companies in our dataset, and the top 10 results were analyzed for correctness and ranking quality.

\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ceo_quality.png}
\caption{CEO queries: correctness \& coverage by search engine}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ceo_correlation.png}
\caption{PhiK correlation, CEO}
\end{subfigure}
\caption{Performance comparison of search engines for CEO queries. Left: Radar chart showing coverage and score metrics for large and small companies. Right: Correlation heatmap showing similarity in result patterns across search engines.}
\label{fig:ceo_analysis}
\end{figure}

The radar chart (Figure \ref{fig:ceo_analysis}, left) illustrates the performance differences across various search engines in terms of correctness and coverage for CEO queries. SerpAPI and SearchAPI generally perform the best, exhibiting high correctness and coverage across both large and small companies. Google Serper and Google also show strong performance but are slightly less consistent. DuckDuckGo and Brave display moderate performance, with correct CEO information being more accessible for smaller companies. Conversely, engines like You, Tavily, Jina, and Exa show slightly lower performance, particularly for the small companies.

The correlation heatmap (Figure \ref{fig:ceo_analysis}, right) reveals two distinct clusters of search engines. Google Serper, Google, SearchAPI, and SerpAPI form a tightly correlated cluster, with SearchAPI and SerpAPI showing perfect correlation, indicating they likely use similar underlying data sources. Brave, DuckDuckGo, Exa, Jina, Tavily, and You form a second cluster with moderate internal correlations, where Tavily and You also show perfect correlation. The near-zero correlations between engines from different clusters indicate fundamentally different retrieval strategies or data sources.

\subsection{Revenue Queries}

Similar patterns emerge for revenue queries. Each search engine was queried with "revenue of [company] in 2024" for companies in our dataset, and the top 10 results were analyzed for correctness and ranking quality.

\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/revenue_quality.png}
\caption{Revenue queries: correctness \& coverage by search engine}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/revenue_correlation.png}
\caption{PhiK correlation, Revenue}
\end{subfigure}
\caption{Performance comparison of search engines for revenue queries. Left: Radar chart showing coverage and score metrics for large and small companies. Right: Correlation heatmap showing similarity in result patterns across search engines.}
\label{fig:revenue_analysis}
\end{figure}

The radar chart (Figure \ref{fig:revenue_analysis}, left) shows that Google Serper achieves the highest performance in terms of accuracy score, while engines like Brave, Duckduckgo, Exa, Jina, Tavily and You shows the highest coverage. 
It also should be mentioned that the seacrh of the revenue appears to be a harder task, because of that we notice the slight drop in the accuracy. Also it is seen that the gap between small and large companies is not so huge in comparison to the CEO search. This can be cause by the fact that this metrics is more stable and have lower variation in the sources.

The correlation heatmap for revenue queries (Figure \ref{fig:revenue_analysis}, right) shows a similar clustering pattern. Google Serper, Google, SearchAPI, and SerpAPI form a highly correlated cluster, with SearchAPI and SerpAPI again showing perfect correlation. Tavily and You show very high correlation, while Jina correlates moderately with Tavily, You, and Exa. Brave shows moderate correlations with Jina, Tavily, and You. It tells us that if we want to use multiple search engines in the final product it might be valuable to get the best engines from different clusters to reach highest coverage.

\subsection{Gross Margin Queries}

Each search engine was queried with "gross margin of [company] in 2024" for companies in our dataset, and the top 10 results were analyzed for correctness and ranking quality.

\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/gross_margin_quality.png}
\caption{Gross margin queries: correctness \& coverage by search engine}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/gross_margin_correlation.png}
\caption{PhiK correlation, Gross Margin}
\end{subfigure}
\caption{Performance comparison of search engines for gross margin queries. Left: Radar chart showing coverage and score metrics for large and small companies. Right: Correlation heatmap showing similarity in result patterns across search engines.}
\label{fig:gross_margin_analysis}
\end{figure}

The radar chart (Figure \ref{fig:gross_margin_analysis}, left) shows lower correlations comparing with the previous search patterns. This may be caused by the highest difficulty of the search task.

\subsection{Comparison of Multiagentic Pipeline and GPT WebSearch Tool}

The second stage of evaluation compared the performance of the multiagentic pipeline (implemented in Metrics Graph and Table Graph) against the GPT WebSearch Tool approach (implemented in Websearch Tool Graph). Both approaches were evaluated on their ability to extract accurate information for CEO, revenue, and gross margin queries, with results analyzed separately for large and small companies.

\begin{table}[H]
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Approach} & \textbf{Size} & \textbf{CEO Accuracy} & \textbf{Revenue Accuracy} & \textbf{Gross Margin Accuracy} \\
\midrule
Multiagentic & Large & 0.8 & 0.6 & 0.29 \\
Multiagentic & Small & 1.0 & 1.0 & 0.2 \\
GPT Web-Search & Large & 1.0 & 1.0 & 0.86 \\
GPT Web-Search & Small & 1.0 & 1.0 & 0.7 \\
\bottomrule
\end{tabular}
\caption{Accuracy comparison between Multiagentic Pipeline and GPT WebSearch Tool approaches across different query types and company sizes.}
\label{tab:approach_comparison}
\end{table}

The results (Table \ref{tab:approach_comparison}) reveal distinct performance patterns for each approach. For CEO queries, both approaches achieve perfect accuracy (1.0) for small companies, while GPT WebSearch Tool shows perfect accuracy (1.0) for large companies compared to 0.8 accuracy for the multiagentic pipeline. For revenue queries, GPT WebSearch Tool maintains perfect accuracy (1.0) for both large and small companies, while the multiagentic pipeline achieves perfect accuracy (1.0) for small companies but only 0.6 accuracy for large companies. The most significant difference emerges in gross margin queries: GPT WebSearch Tool demonstrates 0.86 accuracy for large companies and 0.7 accuracy for small companies, while the multiagentic pipeline shows 0.29 accuracy for large companies and 0.2 accuracy for small companies.

These results suggest that GPT WebSearch Tool generally outperforms the multiagentic pipeline in extraction accuracy, particularly for revenue and gross margin queries. However, the multiagentic pipeline offers advantages in quality control mechanisms such as explicit credibility scoring, hybrid retrieval, and iterative refinement, which may be valuable for applications where source verification and answer completeness are prioritized over raw extraction accuracy. The performance gap is most pronounced for gross margin queries, which appear to be the most challenging task for both approaches, with the multiagentic pipeline struggling more significantly in this domain.

\section{Conclusions}

\subsection{Service Design Achievements}

The multiagentic pipeline successfully addresses the business need for automated financial information extraction by providing:

\begin{enumerate}
    \item \textbf{Automated Research Service}: Transforms manual research processes into an automated service, reducing effort by an estimated 80\%
    \item \textbf{Credibility Assurance}: Explicit credibility scoring and filtering ensure only trustworthy sources are used in financial research
    \item \textbf{Comprehensive Information Gathering}: Iterative refinement loops automatically detect and fill information gaps, ensuring answer completeness
    \item \textbf{Full Traceability}: Complete citation rate for multiagentic workflows ensures all extracted information can be verified
    \item \textbf{Scalable Batch Processing}: Asynchronous table filling mode enables processing of hundreds of company-metric combinations in parallel
\end{enumerate}

\subsection{Methodological Contributions}

\begin{enumerate}
    \item \textbf{Hybrid Retrieval}: The combination of semantic similarity ($\alpha = 0.5$) and credibility scoring (min = 0.5) achieves optimal balance between relevance and trustworthiness, critical for financial data
    \item \textbf{Multi-Agent Orchestration}: Specialized agents (9 total) with model selection based on task complexity (GPT-4o-mini for lightweight, GPT-5 for complex tasks) optimize both cost and performance
    \item \textbf{Iterative Refinement}: The refinement loop automatically improves answer completeness by detecting gaps and triggering additional searches, with maximum 1 retry to balance quality and latency
    \item \textbf{Asynchronous Processing}: Parallel execution of multiple extraction tasks in table filling mode significantly reduces total processing time for batch operations
\end{enumerate}

\subsection{Comparison with Alternative Approaches}

The multiagentic pipeline provides superior quality control compared to simpler approaches like GPT WebSearch Tool:

\begin{enumerate}
    \item \textbf{Explicit Quality Control}: Multi-agent architecture with dedicated grading agents provides explicit quality assurance, compared to implicit model-dependent quality in single-agent approaches
    \item \textbf{Credibility Filtering}: Systematic credibility scoring and filtering ensures only reliable sources are used, critical for financial research
    \item \textbf{Iterative Improvement}: Refinement loops automatically improve answer completeness, while single-pass approaches may miss information
    \item \textbf{Trade-offs}: Multiagentic approach prioritizes source credibility and complete citations over speed, while GPT WebSearch Tool achieves higher extraction accuracy and faster response times
\end{enumerate}

\subsection{Real-World Impact}

The service design enables several practical applications:

\begin{enumerate}
    \item \textbf{Financial Research Firms}: Rapid population of company comparison tables for sector analysis, reducing research time from 8-10 hours to 25-30 minutes
    \item \textbf{Due Diligence}: Automated verification of company information with explicit source credibility, ensuring reliable data for M\&A processes
    \item \textbf{Competitive Intelligence}: Automated maintenance of competitive dashboards with structured outputs suitable for integration with business intelligence tools
\end{enumerate}

\subsection{Limitations and Future Work}

\begin{enumerate}
    \item \textbf{Latency}: Average latency may be prohibitive for real-time applications; parallel processing and caching could reduce this
    \item \textbf{Retry Limit}: Maximum 1 retry may be insufficient for complex queries; adaptive retry limits based on query complexity could improve completeness
    \item \textbf{Vector Store}: In-memory stores are recreated per query; persistent stores with caching could reduce latency
    \item \textbf{Source Diversity}: Current implementation focuses on web sources; integration with proprietary databases and financial APIs could improve accuracy
    \item \textbf{Web Search API Limitations}: Current implementation uses Exa Search API with limited resources; upgrading to a paid API tier with more resources (higher rate limits, more results per query, advanced filtering options) could improve search quality and reduce rate limiting issues
    \item \textbf{Webpage Scraping}: Current implementation relies on API-provided content snippets; improving information scraping to extract full webpage content (including dynamic content, tables, and structured data) would provide more comprehensive source material for answer generation
    \item \textbf{Chunking Strategy}: Current chunking uses RecursiveCharacterTextSplitter with fixed parameters; developing domain-specific chunking strategies (e.g., preserving financial tables, maintaining paragraph coherence, handling multi-column layouts) could improve retrieval quality and reduce information fragmentation
    \item \textbf{Deep Research Agent}: Creating a specialized deep research agent that can chain multiple tool calls iteratively, retrieving and analyzing document chunks as long as needed until sufficient information is gathered, rather than limiting to a single retrieval pass. This would enable more thorough investigation of complex queries
    \item \textbf{Persistent Vector Store}: Implementing a static/persistent vector store that saves parsed webpages and their embeddings, avoiding redundant parsing and embedding computation for previously retrieved documents. This would significantly improve efficiency for repeated queries and reduce API costs
    \item \textbf{External Data Sources}: Adding integration with external non-web information sources (proprietary financial databases, internal company databases, subscription-based data services, APIs from financial data providers) when web sources are insufficient or when higher-quality structured data is required
\end{enumerate}

\subsection{Final Assessment}

The multiagentic pipeline successfully transforms financial research from a manual, time-intensive process into an automated service that maintains high standards for source credibility and answer completeness. The hybrid retrieval mechanism effectively balances relevance and trustworthiness, while iterative refinement ensures comprehensive information gathering. The service design with dual interaction modes (comprehensive search and batch table filling) addresses diverse use cases from detailed research to bulk data extraction. However, experimental evaluation revealed that the websearch tool approach achieved higher extraction accuracy compared to the custom multiagentic pipeline.

Despite the accuracy advantage of the websearch tool approach, the custom multiagentic architecture offers significant strategic benefits that extend beyond raw accuracy metrics. First, the custom approach provides full control over document retrieval and processing pipelines, enabling integration of proprietary and closed data sources (internal databases, subscription services, confidential documents) that cannot be accessed through websearch tools. In websearch tool-based solutions, there is no control over which pages are scraped, and custom or closed data cannot be integrated. Second, websearch tools are not universally available across all LLM providers, and for regulated industries such as banking and financial services, using external cloud-based LLMs like GPT may violate compliance requirements. The custom multiagentic architecture can be deployed with self-hosted LLMs, ensuring data privacy and regulatory compliance while maintaining the same sophisticated workflow capabilities. Third, the modular agentic flow enables unlimited customization possibilities: specialized agents can be added for domain-specific tasks, custom retrieval strategies can be implemented, and the entire pipeline can be adapted to specific organizational needs. Finally, a hybrid approach is possible: combining GPT with websearch tool for initial information gathering (leveraging its accuracy advantage), followed by the multiagentic workflow for credibility assessment, hybrid retrieval, and iterative refinement to prevent hallucinations and ensure source verification. This hybrid strategy leverages the accuracy and speed of websearch tools while maintaining the quality control and hallucination prevention mechanisms of the multiagentic pipeline, offering the best of both approaches.


\end{document}
