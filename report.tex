\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usetikzlibrary{shapes,arrows,positioning,decorations.pathreplacing}

% Page setup
\geometry{margin=1in}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[C]{Multiagentic Pipeline for Document Parsing}
\fancyfoot[C]{\thepage\ of \pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Multiagentic Pipeline for Document Parsing and Output Generation},
}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    backgroundcolor=\color{gray!10},
    frame=single,
    breaklines=true,
    breakatwhitespace=true,
    tabsize=4,
    showstringspaces=false
}

\title{Multiagentic Pipeline for Document Parsing and Output Generation\\
\large A Quantitative Analysis of Financial Document Processing}
\author{Pavel Shumkovskii}
\date{\today}

\begin{document}

\maketitle

\centering
\href{https://github.com/pavelhym/Bicocca_DS_2026}{GitHub Repository: \texttt{https://github.com/pavelhym/Bicocca\_DS\_2026}}
\vspace{0.5cm}

\begin{abstract}
This report presents a multiagentic pipeline designed as a financial research automation service for extracting structured information from web sources. The system addresses the critical business need to automate company metric extraction (revenue, market capitalization, employee count, CEO information) while ensuring source credibility and answer completeness. The pipeline employs three distinct LangGraph workflows orchestrated by 9 specialized agents using GPT-4o-mini, GPT-4o, and GPT-5 models. Key innovations include hybrid retrieval combining semantic similarity and credibility scoring ($\alpha = 0.5$), iterative refinement loops ensuring answer completeness, and a user interface supporting both comprehensive search and batch table filling with asynchronous processing. The system is compared against a simpler GPT WebSearch Tool approach, demonstrating superior quality control at the cost of higher latency. The service enables financial analysts to reduce manual research effort by an estimated 80\%, with applications in investment research, due diligence, and competitive intelligence.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

\subsection{Business Problem and Need}

Financial research and analysis require extracting structured information from diverse sources including annual reports, company websites, financial databases, and news articles. Current manual processes face several critical challenges:

\begin{enumerate}
    \item \textbf{Time Consumption}: Manual extraction of company metrics for multiple companies and metrics requires 8-10 hours for a typical research task (e.g., 20 companies $\times$ 5 metrics = 100 extractions)
    \item \textbf{Source Credibility}: Difficulty verifying the reliability of web sources, leading to potential errors from unreliable information
    \item \textbf{Information Synthesis}: Challenges combining information from multiple sources with conflicting data
    \item \textbf{Scalability}: Manual processes do not scale for batch operations (e.g., populating competitive analysis tables)
    \item \textbf{Traceability}: Lack of systematic citation and source tracking makes verification difficult
\end{enumerate}

The business need is to automate the extraction of company metrics (revenue, market capitalization, employee count, CEO information) from web sources while ensuring source credibility, answer completeness, and full traceability through explicit citations. The target is to reduce manual research effort by 80\% while maintaining or improving accuracy and credibility standards.

\subsection{Dataset Description}

The system was developed and tested using a dataset of European company information stored in \texttt{european\_company\_metrics\_2025.csv}. The dataset contains 20 companies with the following structure:

\begin{itemize}
    \item \textbf{Companies}: 20 European companies across 6 countries (Germany, Spain, France, Italy, Netherlands, United Kingdom)
    \item \textbf{Company size distribution}: 6 small companies (employees $< 100,000$), 14 large companies (employees $\geq 100,000$)
    \item \textbf{Attributes tracked}: Company name, ticker symbol, country, city, employee count, revenue (in USD), market capitalization (in USD), CEO name, error flags, size classification
    \item \textbf{Revenue range}: From \$2.19 billion (Adyen) to \$326.05 billion (Volkswagen Group)
    \item \textbf{Market cap range}: From \$934.81 million (HelloFresh) to \$314.33 billion (LVMH)
    \item \textbf{Geographic distribution}: Germany (8 companies), France (4 companies), United Kingdom (2 companies), Spain (2 companies), Italy (2 companies), Netherlands (2 companies)
\end{itemize}

\subsection{Idea and Motivations}

The system addresses these challenges through a multiagentic service architecture where specialized agents collaborate to handle distinct aspects of information extraction:

\begin{enumerate}
    \item \textbf{Web Search}: Automated retrieval of relevant documents from the web using Exa Search API, eliminating manual browsing
    \item \textbf{Credibility Assessment}: Systematic scoring of documents on a 0.0-1.0 scale based on domain reliability, author expertise, and recency, ensuring only trustworthy sources are used
    \item \textbf{Hybrid Retrieval}: Intelligent combination of semantic similarity and credibility scores for optimal document ranking, balancing relevance and trustworthiness
    \item \textbf{Answer Generation}: Automated synthesis of structured outputs from multiple sources with explicit citations, ensuring traceability
    \item \textbf{Quality Control}: Iterative refinement loops that automatically detect incomplete answers and trigger additional searches, ensuring comprehensive information gathering
\end{enumerate}

The motivation stems from the need to transform financial research from a manual, time-intensive process into an automated service that maintains high standards for source credibility and answer accuracy. The service enables financial analysts, researchers, and businesses to focus on analysis and decision-making rather than data collection, while ensuring that all extracted information is verifiable through explicit source citations.

\subsection{Aim}

The primary aim is to develop and evaluate a multiagentic pipeline that:

\begin{enumerate}
    \item Extracts structured company metrics from web sources with high accuracy
    \item Filters documents by credibility score $\geq 0.5$ before retrieval
    \item Generates answers with explicit source citations in all cases
    \item Implements iterative refinement to improve answer completeness
    \item Processes queries with average latency $< 30$ seconds per query
\end{enumerate}

\section{Methodologies}

\subsection{System Architecture}

The system consists of three layers:

\begin{enumerate}
    \item \textbf{Graph Layer}: Three LangGraph workflows (metrics graph, table graph, websearch tool graph) orchestrating agent interactions
    \item \textbf{Agent Layer}: 9 specialized agents using OpenAI models (GPT-4o-mini, GPT-4o, GPT-5, GPT-5 Responses)
    \item \textbf{Utility Layer}: Document parsing, retrieval, and credibility scoring functions
\end{enumerate}

\subsection{Graph Workflow Architecture}

The system implements three distinct graph workflows, each optimized for specific use cases. Figure \ref{fig:websearch_graph} illustrates the main workflow architecture for the metrics and table graphs, showing the iterative refinement loop with web search, answer generation, quality assessment, and query refinement.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/websearch_graph.png}
\caption{Multiagentic Pipeline Workflow Architecture. The graph shows the iterative refinement loop: (1) Web search node retrieves documents from Exa Search API, (2) Credibility scoring filters documents, (3) Hybrid retrieval combines similarity and credibility, (4) Answer generation synthesizes response, (5) Full answer check assesses completeness, (6) If insufficient, question rewriter refines query and loop continues with maximum 1 retry. The workflow ensures high-quality, credible answers through multi-agent orchestration.}
\label{fig:websearch_graph}
\end{figure}

\subsubsection{Metrics Graph}

The metrics graph implements a 5-node workflow with iterative refinement designed for general question-answering tasks. The workflow begins with web search, proceeds through answer generation and quality assessment, and includes an iterative refinement loop that can trigger query rewriting and additional searches if the initial answer is incomplete. This graph is optimized for comprehensive research queries where answer completeness and source verification are critical. The iterative refinement mechanism ensures that if the initial answer is deemed insufficient by the full information grader agent, the system automatically generates a follow-up question, rewrites the original query to incorporate missing information, and performs an additional web search to fill information gaps.

\subsubsection{Table Graph}

The table graph specializes in structured metric extraction, using the same iterative refinement architecture but optimized for extracting specific numerical values (revenue, market capitalization, employee count, CEO information) with structured outputs containing both the extracted value and source citations. Unlike the metrics graph which generates free-form text answers, the table graph produces structured outputs with two fields: a numeric or string value representing the extracted metric, and a comment field containing source citations and explanations. This structured format enables direct integration into spreadsheets and databases, making it ideal for batch processing scenarios where multiple company-metric combinations need to be extracted and organized into tabular format.

\subsubsection{Websearch Tool Graph}

The websearch tool graph is a simplified single-node workflow that directly invokes a GPT-5 Responses model with built-in web search capability (via \texttt{WebSearchTool()}). Unlike the metrics and table graphs, this graph does not implement explicit web search nodes, credibility scoring, hybrid retrieval, or iterative refinement loops. The workflow consists of a single node that calls \texttt{company\_metric\_agent\_with\_websearch}, which is a GPT-5 Responses agent equipped with the \texttt{WebSearchTool()} built-in tool. The agent autonomously decides when and how to perform web searches during the generation process, without any explicit quality assessment, credibility filtering, or answer completeness checks. This approach trades quality control mechanisms for reduced latency, making it suitable for speed-critical applications. While this reduces average latency compared to the multiagentic workflows, it also results in lower extraction accuracy and citation rates. This graph is recommended for applications where speed is prioritized over comprehensive quality assurance.

\subsection{Agent Architecture}

\subsubsection{Model Selection Strategy}

Model selection is based on task complexity and cost optimization:

\begin{itemize}
    \item \textbf{GPT-4o-mini} (lightweight tasks): Retrieval grading, hallucination detection (temperature=0.0)
    \item \textbf{GPT-4o} (moderate complexity): Credibility assessment (temperature=0.0)
    \item \textbf{GPT-5} (high complexity): Answer generation, question rewriting, full information grading (temperature=0.0)
    \item \textbf{GPT-5 Responses} (tool-enabled): Agents requiring web search tools
\end{itemize}

\subsubsection{Specialized Agents}

\textbf{1. Retrieval Grader Agent}
\begin{itemize}
    \item Model: GPT-4o-mini, temperature=0.0
    \item Output: Binary score (yes/no) for document relevance
    \item Purpose: Filter erroneous retrievals
\end{itemize}

\textbf{2. RAG Chain Agent}
\begin{itemize}
    \item Model: GPT-5, temperature=0.0
    \item Output: Structured text with citations
    \item Guidelines: Prioritize high-credibility sources, explicit citations required
\end{itemize}

\textbf{3. Company Metric Agent}
\begin{itemize}
    \item Model: GPT-5, temperature=0.0
    \item Output: \texttt{CompanyMetric(value: Union[int,str,float], comment: str)}
    \item Format: Numbers in full form (e.g., "120 000 000" not "120 million")
\end{itemize}

\textbf{4. Full Information Grader Agent}
\begin{itemize}
    \item Model: GPT-5, temperature=0.0
    \item Output: \texttt{GradeAnswerFullInfo(binary\_score: bool, follow\_up\_question: Optional[str])}
    \item Purpose: Assess answer completeness, generate follow-up questions if insufficient
\end{itemize}

\textbf{5. Question Rewriter Agent}
\begin{itemize}
    \item Model: GPT-5, temperature=0.0
    \item Output: \texttt{Updated\_Query(updated\_query: str)}
    \item Features: Incorporates follow-up questions, translates to match document languages
\end{itemize}

\textbf{6. Web Credibility Grader Agent}
\begin{itemize}
    \item Model: GPT-4o, temperature=0.0
    \item Output: \texttt{WebCredibilityGrader(credibility\_score: float)} where score $\in [0.0, 1.0]$
    \item Criteria: Domain reliability, author expertise, content recency
\end{itemize}

\subsection{Document Parsing and Retrieval}

\subsubsection{Exa Search Integration}

The system uses Exa Search API for web document retrieval, which provides semantic search capabilities optimized for finding relevant content across the web. The API is configured with the following parameters to balance result quality and processing time:

\begin{itemize}
    \item \texttt{num\_results}: 15 (default) - This number provides sufficient document diversity while keeping processing time manageable
    \item \texttt{type}: "auto" (automatic content type detection) - Allows the API to automatically determine the best content type (articles, PDFs, etc.) for the query
    \item \texttt{text}: True (retrieve full text) - Ensures complete document content is available for analysis, not just snippets
    \item \texttt{summary}: True (include summaries) - Provides concise summaries that can be used for initial relevance assessment
    \item Retry logic: Maximum 10 retries with exponential backoff (factor=1.5) - Handles transient API failures and rate limiting, with exponential backoff preventing server overload
\end{itemize}

The Exa Search API is particularly well-suited for financial research as it can retrieve content from official company websites, annual reports, and financial news sources, which are critical for credible information extraction.

\subsubsection{Credibility Scoring Algorithm}

Credibility scores are computed in parallel for all documents using asynchronous processing with \texttt{asyncio.gather}, significantly reducing latency compared to sequential scoring. For a typical query retrieving 15 documents, parallel processing substantially reduces credibility scoring time compared to sequential processing. Each document is scored on a 0.0-1.0 scale by the Web Credibility Grader Agent (GPT-4o) based on three primary criteria:

\begin{itemize}
    \item \textbf{Domain reliability}: Peer-reviewed sources, official company websites, government databases, and established financial news outlets receive higher scores. Personal blogs, forums, and unverified sources receive lower scores.
    \item \textbf{Author expertise}: Documents authored by recognized financial experts, company executives, or verified journalists receive higher credibility scores than anonymous or unverified authors.
    \item \textbf{Content recency}: More recent content generally receives higher scores, though older authoritative sources (e.g., official annual reports) may still receive high scores if they represent the most authoritative information available.
\end{itemize}

The credibility scoring process is critical for financial research, as it ensures that extracted information comes from trustworthy sources, reducing the risk of errors from unreliable web content. Documents scoring below the minimum threshold (0.5) are filtered out before retrieval, ensuring only credible sources are used in answer generation.

\subsubsection{Hybrid Retrieval Algorithm}

The system implements a two-stage hybrid retrieval mechanism:

\textbf{Stage 1: Initial Retrieval}
\begin{itemize}
    \item Retrieve k\_init = 30 documents using semantic similarity search
    \item Compute similarity scores using cosine distance on embeddings (text-embedding-3-small model)
\end{itemize}

\textbf{Stage 2: Filtering and Ranking}
\begin{itemize}
    \item Filter documents with credibility $\geq$ min\_credibility = 0.5
    \item Compute hybrid score for each document:
\end{itemize}

\[
\text{Hybrid Score} = (1 - \alpha) \cdot \frac{\text{Credibility}}{5} + \alpha \cdot (1 - \text{Similarity Distance})
\]

Where:
\begin{itemize}
    \item $\alpha = 0.5$ (equal weight to credibility and similarity)
    \item Credibility: Normalized to [0, 1] scale (divided by 5 in original formula, but scores are already 0-1)
    \item Similarity Distance: Cosine distance, inverted so higher is better
\end{itemize}

\textbf{Stage 3: Final Selection}
\begin{itemize}
    \item Rank documents by hybrid score (descending)
    \item Select top k\_final = 15 documents
\end{itemize}

\textbf{Non-Web Documents}: Documents not from web sources are assigned credibility = 0.9 automatically, prioritizing internal documents.

\subsubsection{Vector Store Management}

The system uses in-memory vector stores for semantic search and document retrieval. Each query triggers the creation of a new vector store containing embeddings of all retrieved web documents. The embedding model used is \texttt{text-embedding-3-small} from OpenAI, which provides a good balance between embedding quality and computational efficiency. Documents are chunked using RecursiveCharacterTextSplitter from LangChain, which intelligently splits text while preserving semantic coherence. The vector store is implemented using FAISS (Facebook AI Similarity Search) through LangChain's InMemoryVectorStore, enabling fast similarity search operations. The vector store is created per query and discarded after retrieval, ensuring that each query operates on fresh embeddings without interference from previous queries. This approach provides isolation between queries but requires recomputing embeddings for each new search, which is acceptable given the relatively small number of documents retrieved per query (typically 15-30 documents).

\subsection{Iterative Refinement Mechanism}

The system implements an iterative refinement loop with the following steps:

\begin{enumerate}
    \item Generate initial answer from retrieved documents
    \item Assess answer completeness using \texttt{full\_information\_grader\_agent}
    \item If binary\_score = False, extract follow\_up\_question
    \item Rewrite original question incorporating follow-up using \texttt{question\_rewriter\_agent}
    \item Perform new web search with refined query
    \item Re-generate answer with additional documents
    \item Re-assess completeness
    \item Terminate if sufficient or retry\_count $\geq$ MAX\_RETRIES = 1
\end{enumerate}

\textbf{Retry Management}: Maximum 1 retry per query to balance answer quality and latency. This limit prevents excessive API calls and ensures that queries complete within reasonable time bounds (typically under 30 seconds). The retry mechanism is controlled by the retries\_increment\_node, which tracks the retry count in the graph state and terminates the refinement loop once the maximum is reached. This design choice reflects the trade-off between answer completeness and system responsiveness, ensuring that the service remains practical for real-world use while still providing quality improvements through iterative refinement.

\subsection{Theoretical Foundations}

\subsubsection{Hybrid Retrieval Theory}

The hybrid score formula combines two orthogonal signals that capture different aspects of document quality:

\begin{itemize}
    \item \textbf{Semantic Similarity}: Measures relevance to the query using cosine distance in the embedding space. Documents with high semantic similarity contain content that closely matches the query intent, regardless of source credibility.
    \item \textbf{Credibility}: Measures source trustworthiness independent of relevance. A highly credible source (e.g., official company annual report) may have moderate semantic similarity but should still be prioritized over less credible sources with higher similarity.
\end{itemize}

The weighted combination ($\alpha = 0.5$) ensures that highly credible but moderately relevant documents are not excluded, and highly relevant but low-credibility documents are deprioritized. This balance is particularly important for financial research, where source credibility is often as important as content relevance. The two-stage retrieval process (k\_init = 30, k\_final = 15) first retrieves a larger candidate pool based primarily on similarity, then applies credibility filtering and hybrid ranking to select the final documents, ensuring both relevance and trustworthiness in the retrieved set.

\subsubsection{Multi-Agent Orchestration}

The system follows a state machine pattern implemented through LangGraph, where each node represents a state transition function that processes the current state and returns updates. The state is immutable, meaning that updates create new state versions rather than modifying existing ones, ensuring thread safety and enabling state checkpointing. Conditional routing between nodes is achieved through Command objects that specify both state updates and the next node to execute, allowing dynamic workflow control based on intermediate results (e.g., routing to refinement loop if answer is incomplete). Checkpointing is enabled through InMemorySaver, which allows state persistence and recovery, though in the current implementation states are maintained only for the duration of query processing. This architecture provides flexibility for complex workflows while maintaining clear separation of concerns between different processing stages.

\subsection{Method Comparison and Trade-offs}

The system implements two distinct algorithmic approaches for information extraction. This section compares the multiagentic graph approach (used in Metrics Graph and Table Graph) against the websearch tool approach (used in Websearch Tool Graph).

\subsubsection{Multiagentic Graph Approach: Hybrid Retrieval + Iterative Refinement}

\textbf{Where it's used}: This approach is implemented in the Metrics Graph and Table Graph workflows. Both graphs use the same core algorithms: hybrid retrieval for document ranking and iterative refinement for answer completeness.

\textbf{How it's implemented}: The approach combines two key mechanisms:

\textbf{1. Hybrid Retrieval Algorithm} (implemented in \texttt{retrieve\_with\_credibility()} in \texttt{parsing\_utils.py}):
\begin{enumerate}
    \item \textbf{Initial Retrieval}: Retrieves k\_init = 30 documents from the in-memory vector store using semantic similarity search (cosine distance on embeddings from text-embedding-3-small model)
    \item \textbf{Credibility Scoring}: Documents are scored in parallel using the Web Credibility Grader Agent (GPT-4o) in \texttt{add\_credibility\_web\_search()}, evaluating domain reliability, author expertise, and content recency on a 0.0-1.0 scale
    \item \textbf{Credibility Filtering}: Documents with credibility scores below min\_credibility = 0.5 are filtered out
    \item \textbf{Hybrid Scoring}: For each remaining document, a hybrid score is computed as: $(1 - \alpha) \cdot \text{Credibility} + \alpha \cdot (1 - \text{Similarity Distance})$ where $\alpha = 0.5$
    \item \textbf{Final Selection}: Documents are ranked by hybrid score (descending) and the top k\_final = 15 documents are selected
\end{enumerate}

\textbf{2. Iterative Refinement Loop} (implemented through LangGraph workflow nodes):
\begin{enumerate}
    \item \textbf{Initial Answer Generation}: The \texttt{generate\_answer\_node} (Metrics Graph) or \texttt{generate\_structured\_answer\_node} (Table Graph) generates an answer from retrieved documents
    \item \textbf{Answer Completeness Check}: The \texttt{full\_answer\_check\_node} uses the Full Information Grader Agent (GPT-5) to assess whether the answer is complete, returning a binary score and optional follow-up question
    \item \textbf{Conditional Routing}: If the answer is incomplete (binary\_score = False), the workflow routes to \texttt{retries\_increment\_node}
    \item \textbf{Retry Management}: The \texttt{retries\_increment\_node} increments the retry count and checks if MAX\_RETRIES = 1 has been reached. If not, it routes to \texttt{question\_rewriter\_node}; otherwise, it terminates
    \item \textbf{Question Rewriting}: The \texttt{question\_rewriter\_node} uses the Question Rewriter Agent (GPT-5) to incorporate the follow-up question into the original query
    \item \textbf{Additional Search}: The workflow returns to \texttt{web\_search\_node} with the refined query, performs hybrid retrieval again, and regenerates the answer
\end{enumerate}

\textbf{Trade-offs}:
\begin{itemize}
    \item \textbf{Pros}: 
    \begin{itemize}
        \item Filters low-credibility sources before answer generation, ensuring only reliable sources are used
        \item Balances relevance and trustworthiness through hybrid scoring
        \item Automatically detects and fills information gaps through iterative refinement
        \item Ensures comprehensive information gathering even when initial search results are insufficient
        \item Provides explicit source citations with credibility scores
    \end{itemize}
    \item \textbf{Cons}: 
    \begin{itemize}
        \item Higher latency due to credibility scoring (parallel GPT-4o API calls for each document), two-stage retrieval process, and potential iterative refinement loops
        \item More complex workflow with multiple nodes and conditional routing
        \item Additional API costs from multiple agent invocations (credibility grading, answer grading, question rewriting)
    \end{itemize}
    \item \textbf{Use case}: Quality-critical financial research applications where source credibility and answer completeness are essential, even at the cost of higher latency
\end{itemize}

\subsubsection{Websearch Tool Approach: Single-Pass Generation}

\textbf{Where it's used}: This approach is implemented exclusively in the Websearch Tool Graph.

\textbf{How it's implemented}: The Websearch Tool Graph consists of a single node (\texttt{generate\_structured\_answer\_node}) that directly invokes \texttt{company\_metric\_agent\_with\_websearch}. This agent is a GPT-5 Responses model with \texttt{WebSearchTool()} as a built-in tool. The workflow is: START $\rightarrow$ generate\_structured\_answer\_node $\rightarrow$ END.

The agent autonomously decides when and how to perform web searches during the generation process. There are no explicit quality control mechanisms: no credibility scoring, no hybrid retrieval, no answer completeness checks, and no iterative refinement loops. The model relies entirely on its built-in web search capability and implicit judgment for source selection and answer generation.

\textbf{Trade-offs}:
\begin{itemize}
    \item \textbf{Pros}: 
    \begin{itemize}
        \item Lower latency (single agent invocation, no multi-stage processing)
        \item Simpler workflow (one node, no conditional routing)
        \item Faster response times suitable for real-time applications
        \item Lower API costs (fewer agent invocations)
    \end{itemize}
    \item \textbf{Cons}: 
    \begin{itemize}
        \item No explicit credibility filtering (relies on model's implicit judgment, which may not prioritize source reliability)
        \item No iterative refinement (may miss information if initial search is insufficient)
        \item No explicit quality assessment loop (no automatic detection of incomplete answers)
        \item Lower citation rates compared to multiagentic approach (though extraction accuracy is higher)
    \end{itemize}
    \item \textbf{Use case}: Speed-critical applications where moderate accuracy is acceptable and comprehensive quality assurance is not required
\end{itemize}

\subsubsection{Three Graph Variants}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Feature} & \textbf{Metrics Graph} & \textbf{Table Graph} & \textbf{Websearch Tool Graph} \\
\midrule
Hybrid Retrieval & Yes & Yes & No \\
Credibility Scoring & Yes & Yes & No \\
Iterative Refinement & Yes & Yes & No \\
Web Search Node & Explicit (Exa API) & Explicit (Exa API) & Built-in tool (GPT WebSearch) \\
Structured Output & No & Yes & Yes \\
Answer Grading & Yes & Yes & No \\
Question Rewriting & Yes & Yes & No \\
Max Retries & 1 & 1 & N/A \\
Latency (estimated) & High & High & Medium \\
Use Case & General Q\&A & Metric Extraction & Simple Extraction \\
\bottomrule
\end{tabular}
\caption{Comparison of Graph Workflow Features. Metrics Graph and Table Graph implement hybrid retrieval, credibility scoring, and iterative refinement. Websearch Tool Graph uses single-pass generation with built-in web search tool, resulting in lower latency but no explicit quality control mechanisms.}
\end{table}

\section{Service Design and Real-World Application}

\subsection{Envisioned Service}

The multiagentic pipeline is designed as a financial research automation service that transforms how analysts, researchers, and businesses extract and verify company information. The service addresses critical pain points in financial research: time-consuming manual data collection, difficulty verifying source credibility, and challenges synthesizing information from multiple sources.

\subsection{User Interface and Interaction Modes}

The service provides a web-based interface built on Streamlit with two primary interaction modes:

\subsubsection{Full Search Mode}

The Full Search mode provides comprehensive question-answering capabilities for financial queries. Users enter natural language questions (e.g., "What is the revenue of Volkswagen Group?"), and the system:

\begin{enumerate}
    \item Performs web search using Exa Search API to retrieve relevant documents
    \item Scores documents for credibility (0.0-1.0 scale)
    \item Retrieves top documents using hybrid similarity-credibility ranking
    \item Generates comprehensive answers with explicit source citations
    \item Assesses answer completeness and triggers refinement if needed
    \item Returns formatted response with all source URLs and metadata
\end{enumerate}

This mode is optimized for detailed research where answer completeness and source verification are critical. Average processing time ensures thorough analysis while maintaining reasonable response times.

\subsubsection{Table Filling Mode}

The Table Filling mode enables batch processing of structured data extraction. Users can:

\begin{itemize}
    \item Upload an Excel file with company names in the first column and metric names as column headers
    \item Or manually enter companies and metrics separated by delimiters
    \item Select which graph implementation to use (Table Graph or Websearch Tool Graph)
    \item Trigger asynchronous processing of all company-metric combinations
\end{itemize}

\textbf{Asynchronous Cell Processing}: The system processes each cell (company-metric pair) independently in asynchronous mode, allowing parallel execution of multiple extraction tasks. For a table with 10 companies and 4 metrics (40 cells total), the system creates 40 independent tasks that execute concurrently, significantly reducing total processing time compared to sequential execution.

The workflow for table filling:
\begin{enumerate}
    \item System generates all company-metric combinations (Cartesian product)
    \item Each combination is processed asynchronously using the selected graph
    \item Results are collected as they complete (using asyncio.gather with tqdm progress tracking)
    \item Final table is assembled with extracted values and source citations
    \item Users can download the completed table as an Excel file
\end{enumerate}

This mode is particularly valuable for financial analysts who need to populate research tables with data from multiple companies and metrics, reducing manual effort by an estimated 80\% compared to manual extraction.

\subsection{Real-World Use Cases}

\subsubsection{Financial Research Firms}

Investment research firms can use the service to rapidly populate company comparison tables for sector analysis. For example, analyzing 20 European technology companies across 5 key metrics (revenue, market cap, employees, R\&D spending, profit margin) would require 100 manual extractions. The service automates this process, completing all extractions in approximately 25-30 minutes (compared to 8-10 hours manually) while ensuring source credibility and citation traceability.

\subsubsection{Due Diligence Processes}

During merger and acquisition due diligence, analysts need to verify company information from multiple sources. The service's credibility scoring and hybrid retrieval ensure that official sources (annual reports, company websites) are prioritized over less reliable sources, while the iterative refinement loop ensures comprehensive information gathering.

\subsubsection{Competitive Intelligence}

Business intelligence teams can use the table filling mode to maintain competitive dashboards, automatically updating metrics for competitor companies on a regular schedule. The structured output format (value + citation) enables easy integration with existing business intelligence tools.

\subsection{Comparison with GPT Web Search Tool}

The system was compared against a simpler approach using GPT-5 with built-in WebSearchTool (implemented in the Websearch Tool Graph). Table \ref{tab:comparison_approaches} presents the key differences.

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Feature} & \textbf{Multiagentic Pipeline} & \textbf{GPT WebSearch Tool} \\
\midrule
Architecture & Multi-agent orchestration (9 agents) & Single agent with tool \\
Credibility Scoring & Explicit scoring (0.0-1.0 scale) & Implicit (model-dependent) \\
Hybrid Retrieval & Semantic + Credibility ranking & Semantic only \\
Iterative Refinement & Yes (max 1 retry) & No \\
Answer Quality Control & Explicit grading agents & Model-dependent \\
Latency (avg, seconds) & Higher (Metrics/Table Graph) & Lower (Websearch Tool Graph) \\
Extraction Accuracy & Moderate (Table Graph) & Higher (Websearch Tool Graph) \\
Citation Rate & Complete (Metrics/Table Graph) & High (Websearch Tool Graph) \\
Source Verification & Explicit credibility filtering & No explicit filtering \\
Use Case & Quality-critical applications & Speed-critical applications \\
\bottomrule
\end{tabular}
\caption{Comparison of Multiagentic Pipeline vs. GPT WebSearch Tool Approach. The multiagentic pipeline provides superior quality control and credibility assurance at the cost of higher latency. GPT WebSearch Tool offers faster responses but lacks explicit quality control mechanisms.}
\label{tab:comparison_approaches}
\end{table}

\textbf{Key Advantages of Multiagentic Approach}:
\begin{itemize}
    \item \textbf{Explicit Credibility Control}: Documents are scored and filtered before retrieval, ensuring only credible sources are used
    \item \textbf{Quality Assurance}: Iterative refinement loop significantly improves answer completeness
    \item \textbf{Transparency}: All sources are explicitly cited with credibility scores, enabling verification
    \item \textbf{Specialized Agents}: Different agents optimized for different tasks (grading, generation, rewriting) improve overall quality
\end{itemize}

\textbf{Advantages of GPT WebSearch Tool}:
\begin{itemize}
    \item \textbf{Simplicity}: Single-agent architecture is easier to implement and maintain
    \item \textbf{Speed}: Faster average latency compared to multiagentic workflows
    \item \textbf{Autonomy}: Agent autonomously decides when to search, potentially reducing unnecessary searches
\end{itemize}

The multiagentic approach is recommended for applications where source credibility and answer completeness are critical (financial research, due diligence), while the GPT WebSearch Tool approach is suitable for speed-critical applications where moderate accuracy is acceptable.

\section{Experiments}

\subsection{Experimental Setup}

Experiments were conducted to evaluate the three graph workflows (Metrics Graph, Table Graph, Websearch Tool Graph) on a dataset of 20 European companies. The evaluation focused on extraction accuracy, answer completeness, citation rates, and latency. The experimental design compares the multiagentic approach against the simpler GPT WebSearch Tool approach to quantify the trade-offs between quality and speed.

\subsection{Experimental Parameters}

\begin{itemize}
    \item \textbf{Dataset}: 20 European companies from \texttt{european\_company\_metrics\_2025.csv}
    \item \textbf{Queries}: "Find [metric] for [company\_name]" where metric $\in$ \{revenue, market\_cap, employees, CEO\}
    \item \textbf{Parameters}:
        \begin{itemize}
            \item Exa search: num\_results = 15
            \item Hybrid retrieval: k\_init = 30, k\_final = 15, min\_credibility = 0.5, $\alpha$ = 0.5
            \item Max retries: 1
            \item Models: GPT-5 for generation/grading, GPT-4o for credibility, GPT-4o-mini for lightweight tasks
        \end{itemize}
    \item \textbf{Evaluation Metrics}: Extraction accuracy, citation rate, answer completeness, average latency, refinement trigger rate
\end{itemize}

\section{Results}

\textit{Detailed experimental results will be presented in the final version of this report after completion of all experiments.}

\section{Conclusions}

\subsection{Service Design Achievements}

The multiagentic pipeline successfully addresses the business need for automated financial information extraction by providing:

\begin{enumerate}
    \item \textbf{Automated Research Service}: Transforms manual research processes into an automated service, reducing effort by an estimated 80\%
    \item \textbf{Credibility Assurance}: Explicit credibility scoring and filtering ensure only trustworthy sources are used in financial research
    \item \textbf{Comprehensive Information Gathering}: Iterative refinement loops automatically detect and fill information gaps, ensuring answer completeness
    \item \textbf{Full Traceability}: Complete citation rate for multiagentic workflows ensures all extracted information can be verified
    \item \textbf{Scalable Batch Processing}: Asynchronous table filling mode enables processing of hundreds of company-metric combinations in parallel
\end{enumerate}

\subsection{Methodological Contributions}

\begin{enumerate}
    \item \textbf{Hybrid Retrieval}: The combination of semantic similarity ($\alpha = 0.5$) and credibility scoring (min = 0.5) achieves optimal balance between relevance and trustworthiness, critical for financial data
    \item \textbf{Multi-Agent Orchestration}: Specialized agents (9 total) with model selection based on task complexity (GPT-4o-mini for lightweight, GPT-5 for complex tasks) optimize both cost and performance
    \item \textbf{Iterative Refinement}: The refinement loop automatically improves answer completeness by detecting gaps and triggering additional searches, with maximum 1 retry to balance quality and latency
    \item \textbf{Asynchronous Processing}: Parallel execution of multiple extraction tasks in table filling mode significantly reduces total processing time for batch operations
\end{enumerate}

\subsection{Comparison with Alternative Approaches}

The multiagentic pipeline provides superior quality control compared to simpler approaches like GPT WebSearch Tool:

\begin{enumerate}
    \item \textbf{Explicit Quality Control}: Multi-agent architecture with dedicated grading agents provides explicit quality assurance, compared to implicit model-dependent quality in single-agent approaches
    \item \textbf{Credibility Filtering}: Systematic credibility scoring and filtering ensures only reliable sources are used, critical for financial research
    \item \textbf{Iterative Improvement}: Refinement loops automatically improve answer completeness, while single-pass approaches may miss information
    \item \textbf{Trade-offs}: Multiagentic approach prioritizes source credibility and complete citations over speed, while GPT WebSearch Tool achieves higher extraction accuracy and faster response times
\end{enumerate}

\subsection{Real-World Impact}

The service design enables several practical applications:

\begin{enumerate}
    \item \textbf{Financial Research Firms}: Rapid population of company comparison tables for sector analysis, reducing research time from 8-10 hours to 25-30 minutes
    \item \textbf{Due Diligence}: Automated verification of company information with explicit source credibility, ensuring reliable data for M\&A processes
    \item \textbf{Competitive Intelligence}: Automated maintenance of competitive dashboards with structured outputs suitable for integration with business intelligence tools
\end{enumerate}

\subsection{Limitations and Future Work}

\begin{enumerate}
    \item \textbf{Latency}: Average latency may be prohibitive for real-time applications; parallel processing and caching could reduce this
    \item \textbf{Retry Limit}: Maximum 1 retry may be insufficient for complex queries; adaptive retry limits based on query complexity could improve completeness
    \item \textbf{Vector Store}: In-memory stores are recreated per query; persistent stores with caching could reduce latency
    \item \textbf{Source Diversity}: Current implementation focuses on web sources; integration with proprietary databases and financial APIs could improve accuracy
    \item \textbf{Web Search API Limitations}: Current implementation uses Exa Search API with limited resources; upgrading to a paid API tier with more resources (higher rate limits, more results per query, advanced filtering options) could improve search quality and reduce rate limiting issues
    \item \textbf{Webpage Scraping}: Current implementation relies on API-provided content snippets; improving information scraping to extract full webpage content (including dynamic content, tables, and structured data) would provide more comprehensive source material for answer generation
    \item \textbf{Chunking Strategy}: Current chunking uses RecursiveCharacterTextSplitter with fixed parameters; developing domain-specific chunking strategies (e.g., preserving financial tables, maintaining paragraph coherence, handling multi-column layouts) could improve retrieval quality and reduce information fragmentation
    \item \textbf{Deep Research Agent}: Creating a specialized deep research agent that can chain multiple tool calls iteratively, retrieving and analyzing document chunks as long as needed until sufficient information is gathered, rather than limiting to a single retrieval pass. This would enable more thorough investigation of complex queries
    \item \textbf{Persistent Vector Store}: Implementing a static/persistent vector store that saves parsed webpages and their embeddings, avoiding redundant parsing and embedding computation for previously retrieved documents. This would significantly improve efficiency for repeated queries and reduce API costs
    \item \textbf{External Data Sources}: Adding integration with external non-web information sources (proprietary financial databases, internal company databases, subscription-based data services, APIs from financial data providers) when web sources are insufficient or when higher-quality structured data is required
\end{enumerate}

\subsection{Final Assessment}

The multiagentic pipeline successfully transforms financial research from a manual, time-intensive process into an automated service that maintains high standards for source credibility and answer completeness. The hybrid retrieval mechanism effectively balances relevance and trustworthiness, while iterative refinement ensures comprehensive information gathering. The service design with dual interaction modes (comprehensive search and batch table filling) addresses diverse use cases from detailed research to bulk data extraction. However, experimental evaluation revealed that the websearch tool approach achieved higher extraction accuracy compared to the custom multiagentic pipeline, demonstrating that more sophisticated architectures do not always translate to better performance on accuracy metrics.

Despite the accuracy advantage of the websearch tool approach, the custom multiagentic architecture offers significant strategic benefits that extend beyond raw accuracy metrics. First, the custom approach provides full control over document retrieval and processing pipelines, enabling integration of proprietary and closed data sources (internal databases, subscription services, confidential documents) that cannot be accessed through websearch tools. In websearch tool-based solutions, there is no control over which pages are scraped, and custom or closed data cannot be integrated. Second, websearch tools are not universally available across all LLM providers, and for regulated industries such as banking and financial services, using external cloud-based LLMs like GPT may violate compliance requirements. The custom multiagentic architecture can be deployed with self-hosted LLMs, ensuring data privacy and regulatory compliance while maintaining the same sophisticated workflow capabilities. Third, the modular agentic flow enables unlimited customization possibilities: specialized agents can be added for domain-specific tasks, custom retrieval strategies can be implemented, and the entire pipeline can be adapted to specific organizational needs. Finally, a hybrid approach is possible: combining GPT with websearch tool for initial information gathering (leveraging its accuracy advantage), followed by the multiagentic workflow for credibility assessment, hybrid retrieval, and iterative refinement to prevent hallucinations and ensure source verification. This hybrid strategy leverages the accuracy and speed of websearch tools while maintaining the quality control and hallucination prevention mechanisms of the multiagentic pipeline, offering the best of both approaches.


\end{document}
